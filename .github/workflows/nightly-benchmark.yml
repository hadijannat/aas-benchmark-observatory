name: Monthly Benchmark

on:
  schedule:
    - cron: "0 3 1 * *"
  workflow_dispatch:
    inputs:
      sdk_filter:
        description: "Run only this SDK id (leave empty for all)"
        required: false
        default: ""

permissions:
  pages: write
  id-token: write

concurrency:
  group: benchmark
  cancel-in-progress: false

jobs:
  # ── Job 1: Build matrices from known-sdks.json ────────────────
  matrix:
    runs-on: ubuntu-latest
    outputs:
      sdk_matrix: ${{ steps.set.outputs.sdk_matrix }}
      server_matrix: ${{ steps.set.outputs.server_matrix }}
    steps:
      - uses: actions/checkout@v4

      - name: Generate matrices
        id: set
        env:
          SDK_FILTER: ${{ github.event.inputs.sdk_filter }}
        run: |
          SDK_MATRIX=$(bash scripts/set-matrix.sh sdk)
          if [ -n "$SDK_FILTER" ]; then
            SDK_MATRIX=$(echo "$SDK_MATRIX" | jq -c --arg id "$SDK_FILTER" \
              '{include: [.include[] | select(.id == $id)]}')
          fi
          echo "sdk_matrix=$SDK_MATRIX" >> "$GITHUB_OUTPUT"

          SERVER_MATRIX=$(bash scripts/set-matrix.sh server)
          if [ -n "$SDK_FILTER" ]; then
            SERVER_MATRIX=$(echo "$SERVER_MATRIX" | jq -c --arg id "$SDK_FILTER" \
              '{include: [.include[] | select(.id == $id)]}')
          fi
          echo "server_matrix=$SERVER_MATRIX" >> "$GITHUB_OUTPUT"

  # ── Job 2a: Benchmark each SDK library ───────────────────────
  sdk-benchmark:
    needs: matrix
    runs-on: ubuntu-latest
    strategy:
      matrix: ${{ fromJSON(needs.matrix.outputs.sdk_matrix) }}
      max-parallel: 2
      fail-fast: false

    steps:
      - uses: actions/checkout@v4

      - name: Prepare output directory
        run: mkdir -p results/${{ matrix.id }}

      - name: Collect environment metadata
        run: bash harness/collect-env.sh > results/${{ matrix.id }}/env.json

      - name: Generate datasets
        run: |
          python3 datasets/generate.py --output-dir datasets/generated
          python3 datasets/generate.py --output-dir datasets/generated --xml
          python3 datasets/generate.py --output-dir datasets/generated --validation-targets
          python3 datasets/generate.py --output-dir datasets/generated --aasx

      - name: Set up Python
        if: matrix.language == 'python'
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Set up Go
        if: matrix.language == 'go'
        uses: actions/setup-go@v5
        with:
          go-version: "1.22"

      - name: Set up .NET
        if: matrix.language == 'csharp'
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: "8.0"

      - name: Set up Node.js
        if: matrix.language == 'typescript'
        uses: actions/setup-node@v4
        with:
          node-version: "20"

      - name: Set up Java
        if: matrix.language == 'java'
        uses: actions/setup-java@v4
        with:
          distribution: "temurin"
          java-version: "17"

      - name: Set up Rust
        if: matrix.language == 'rust'
        uses: dtolnay/rust-toolchain@stable

      - name: Run benchmarks
        run: bash ${{ matrix.adapter_dir }}/run-benchmarks.sh datasets/generated results/${{ matrix.id }}

      - name: Validate report output
        run: python3 scripts/validate_report.py results/${{ matrix.id }}/report.json

      - name: Upload results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: results-${{ matrix.id }}
          path: results/${{ matrix.id }}/

  # ── Job 2b: Benchmark each AAS server ───────────────────────
  server-benchmark:
    needs: matrix
    runs-on: ubuntu-latest
    strategy:
      matrix: ${{ fromJSON(needs.matrix.outputs.server_matrix) }}
      max-parallel: 1
      fail-fast: false

    steps:
      - uses: actions/checkout@v4

      - name: Install aas-test-engines
        run: pip install aas-test-engines

      - name: Install yq
        run: |
          sudo wget -qO /usr/local/bin/yq https://github.com/mikefarah/yq/releases/latest/download/yq_linux_amd64
          sudo chmod +x /usr/local/bin/yq

      - uses: grafana/setup-k6-action@v1

      - name: Prepare output directory
        run: mkdir -p results/${{ matrix.id }}

      - name: Collect environment metadata
        run: bash harness/collect-env.sh > results/${{ matrix.id }}/env.json

      - name: Start services
        working-directory: ${{ matrix.adapter_dir }}
        run: docker compose up -d --wait --wait-timeout 180

      - name: Wait for health
        run: |
          HEALTH_URL=$(yq '.health.url' ${{ matrix.adapter_dir }}/sdk.yaml)
          bash harness/wait-for-health.sh "$HEALTH_URL" 180

      - name: Dump container logs on failure
        if: failure()
        working-directory: ${{ matrix.adapter_dir }}
        run: docker compose logs --no-color

      - name: Seed test data
        run: |
          API_BASE=$(yq '.api_base_url' ${{ matrix.adapter_dir }}/sdk.yaml)
          bash harness/seed-test-data.sh "$API_BASE"

      - name: Run conformance tests
        id: conformance
        continue-on-error: true
        run: |
          bash harness/conformance/run_aas_test_engines.sh \
            ${{ matrix.adapter_dir }}/sdk.yaml \
            results/${{ matrix.id }}

      - name: Validate conformance execution
        run: |
          python3 - <<'PY'
          import json
          import sys
          from pathlib import Path

          path = Path("results/${{ matrix.id }}/conformance_summary.json")
          if not path.exists():
              print(f"Missing conformance summary: {path}", file=sys.stderr)
              sys.exit(1)

          data = json.loads(path.read_text(encoding="utf-8"))
          if data.get("failure_state") == "execution_failed":
              print("Conformance execution failed (command/parsing error).", file=sys.stderr)
              sys.exit(1)
          print("Conformance execution summary OK.")
          PY

      - name: Run k6 scenario benchmarks
        run: |
          API_BASE=$(yq '.api_base_url' ${{ matrix.adapter_dir }}/sdk.yaml)
          k6 run \
            -e BASE_URL="$API_BASE" \
            -e SDK_ID="${{ matrix.id }}" \
            -e OUTPUT_DIR="results/${{ matrix.id }}" \
            harness/k6/scenarios.js

      - name: Run k6 CRUD benchmarks
        run: |
          API_BASE=$(yq '.api_base_url' ${{ matrix.adapter_dir }}/sdk.yaml)
          k6 run \
            -e BASE_URL="$API_BASE" \
            -e SDK_ID="${{ matrix.id }}" \
            -e OUTPUT_DIR="results/${{ matrix.id }}" \
            harness/k6/crud.js

      - name: Tear down services
        if: always()
        working-directory: ${{ matrix.adapter_dir }}
        run: docker compose down -v

      - name: Upload results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: results-${{ matrix.id }}
          path: results/${{ matrix.id }}/

  # ── Job 3: Aggregate & deploy to Pages ───────────────────────
  deploy:
    needs: [sdk-benchmark, server-benchmark]
    if: always()
    runs-on: ubuntu-latest
    environment:
      name: github-pages
      url: ${{ steps.deploy.outputs.page_url }}

    steps:
      - uses: actions/checkout@v4

      - name: Download all result artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: results-*
          path: results/
          merge-multiple: false

      - name: Flatten artifact directories
        run: |
          # download-artifact creates results/results-<id>/<files>
          # We need results/<id>/<files>
          for dir in results/results-*; do
            sdk_id="${dir#results/results-}"
            mv "$dir" "results/$sdk_id" 2>/dev/null || true
          done

      - name: Download previous results for regression detection
        continue-on-error: true
        run: |
          curl -sSf "https://hadijannat.github.io/aas-benchmark-observatory/data/results.json" \
            -o previous_results.json || echo '{}' > previous_results.json

      - name: Aggregate results
        run: |
          python3 scripts/aggregate.py \
            --previous-results previous_results.json

      - name: Prepare Pages content
        run: cp -r dashboard _site

      - name: Upload Pages artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: _site

      - name: Deploy to GitHub Pages
        id: deploy
        uses: actions/deploy-pages@v4
